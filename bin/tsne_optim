import pandas as pd
import numpy as np
import networkx as nx
from sklearn.manifold import MDS, TSNE
from sklearn.neighbors import DistanceMetric

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import requests
import io
import re
import time
import itertools
from collections import Counter
#import multiprocessing
import matplotlib.cm as cm
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from optparse import OptionParser

def get_cluster(X, nclust):
    silhouette_avg = [] 
    clabels = []
    for num_clusters in range(2,nclust):
        km = KMeans(n_clusters=num_clusters,
                    n_init=10,                        # number of iterations with different seeds
                    random_state=1                    # fixes the seed 
                   )
        cluster_labels = km.fit_predict(X)
        clabels.append(cluster_labels)
        # The silhouette_score gives the average value for all the samples.
        # This gives a perspective into the density and separation of the formed
        # clusters
        silhouette_avg.append(silhouette_score(X, cluster_labels))
    return {'silhouette_avg': silhouette_avg, 'cluster_labels': clabels }



def get_caccuracy(clusters, gnps, metac, method='mean'):
    caccuracy = []
    for clabel in clusters['cluster_labels']:
        gnps['clabel'] = clabel 
        g = gnps.groupby(['clabel']) 
        if method=='mean':
            caccuracy.append(g.apply(lambda a: max(Counter(a[metac]).values())/len(a) ).mean())
        elif method=='median':
            caccuracy.append(g.apply(lambda a: max(Counter(a[metac]).values())/len(a) ).median())
        gnps.drop(['clabel'], axis=1, inplace=True) 
    daccurary = {}
    daccurary['maxSilhoutte'] = max(clusters['silhouette_avg']) 
    daccurary['maxSilhoutteN'] = np.where(np.array(clusters['silhouette_avg'])==daccurary['maxSilhoutte'])[0][0]+2
    daccurary['maxCaccuracy'] = max(caccuracy) 
    maxCaccuracyPos = np.where(np.array(caccuracy)==daccurary['maxCaccuracy'])[0][0]
    daccurary['maxCaccuracySilhoutte'] = clusters['silhouette_avg'][maxCaccuracyPos]
    daccurary['maxCaccuracyN'] = maxCaccuracyPos+2
    return daccurary 

def plot_silhouette(X, n_clusters, silhouette_avg, lpar, show=True):
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)
    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])
    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)
    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)
    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)
        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples
    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")
    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')
    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')
    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')
    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")
    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters + '\n'+
		  "n_iter=%d, perplexity=%d, learning_rate=%d" % lpar),
                 fontsize=14, fontweight='bold')
    if show:
        plt.show()
    else:
        return [fig, ax1, ax2]


def main():
    parser = OptionParser(usage="usage: %prog [options]",
                          version="%prog 1.0")
    parser.add_option("-t", "--taskid",
                      help="GNPS taskid")
    parser.add_option("-w", "--workflow",
                      default='MZmine',
                      help="GNPS workflow: MZmine, V1, V2",)
    parser.add_option("-p", "--projection",
                      default='MDS',
                      help="Projection method",)
    parser.add_option("-I", "--iter",
                      default='250,250,1000',
                      help="Number iterations parameter range",)
    parser.add_option("-P", "--perp",
                      default='6,30,100',
                      help="Perplexity parameter range",)
    parser.add_option("-L", "--learn",
                      default='200,100,300',
                      help="Learnig rate parameter range",)
    parser.add_option("-i", "--input",
                      default='Cosine',
                      help="Input for projection: Cosine, Feature intensities",)
    parser.add_option("-M", "--meta",
                      default='',
                      help="Optional metadata file",)
    parser.add_option("-C", "--metac",
                      default='superclass_name',
                      help="Metadata column label",)
    parser.add_option("-F", "--fig",
                      default=1,
                      help="Plot espected layout",)
    (options, args) = parser.parse_args()

    taskid = options.taskid.split(',') 
    workflow = options.workflow 
    projection = options.projection
    minput = options.input
    metac = options.metac

    anno = pd.read_table(options.meta) 

    if workflow=='MZmine':
        url_to_attributes = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=clusterinfo_summary/" % (taskid[0])
        url_to_edges = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=networking_pairs_results_file_filtered/" % (taskid[0])
        url_to_features = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=quantification_table/" % (taskid[0])
        gnps = pd.read_table(io.StringIO(requests.get(url_to_attributes).text))
        net = pd.read_table(io.StringIO(requests.get(url_to_edges).text))
        if len(taskid) > 1:
            url_to_attributes = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=clusterinfo_summary/" % (taskid[1])
            url_to_edges = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=networking_pairs_results_file_filtered/" % (taskid[1])
            gnps1 = pd.read_table(io.StringIO(requests.get(url_to_attributes).text))
            net1 = pd.read_table(io.StringIO(requests.get(url_to_edges).text))
    elif workflow=='V2':
        url_to_attributes = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=clusterinfosummarygroup_attributes_withIDs_withcomponentID/" % (taskid[0])
        url_to_edges = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=networkedges_selfloop/" % (taskid[0])
        gnps = pd.read_table(io.StringIO(requests.get(url_to_attributes).text))
        net = pd.read_table(io.StringIO(requests.get(url_to_edges).text))
        if len(taskid) > 1:
            url_to_attributes = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=clusterinfosummarygroup_attributes_withIDs_withcomponentID/" % (taskid[1])
            url_to_edges = "http://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=networkedges_selfloop/" % (taskid[1])
            gnps1 = pd.read_table(io.StringIO(requests.get(url_to_attributes).text))
            net1 = pd.read_table(io.StringIO(requests.get(url_to_edges).text))

    nclust = len(set(net1['ComponentIndex']))
    # force max num clusters nun classes?
    nclust = len(set(anno[metac]))

    nlist = list(set(net['CLUSTERID1'].tolist()+net['CLUSTERID2'].tolist()))
    nlist.sort()
    nn = len(nlist)
    ndict = {}
    for n in range(nn):
        ndict[nlist[n]] = n
    if minput=='Cosine': 
        m = np.empty([nn,nn])
        m[:nn, :nn] = 0
        for i in net.index:
            m[ndict[net.loc[i, 'CLUSTERID1']], ndict[net.loc[i, 'CLUSTERID2']]] = net.loc[i, 'Cosine']  
            m[ndict[net.loc[i, 'CLUSTERID2']], ndict[net.loc[i, 'CLUSTERID1']]] = net.loc[i, 'Cosine']  
        m = 1-m
    elif minput=='Feature intensities': 
        features = pd.read_csv(io.StringIO(requests.get(url_to_features).text))
        features.index = features['row ID'] 
        dist = DistanceMetric.get_metric('canberra')
        m = dist.pairwise(features.loc[nlist][features.columns[3:]]) 

    staskid = taskid[0][:10]

    #list(range(250, 1000+250, 250)) 
    #niter = [250, 1000, 1000]
    #perp = [2, 6, 30, 50, 100] 
    #lrate = [200, 300]
    niter = list(map(int, options.iter.split(',')))
    niter = list(range(niter[0], niter[2]+niter[1], niter[1])) 
    perp = list(map(int, options.perp.split(',')))
    perp = list(range(perp[0], perp[2]+perp[1], perp[1])) 
    learn = list(map(int, options.learn.split(',')))
    learn = list(range(learn[0], learn[2]+learn[1], learn[1])) 

    parcomb = list(itertools.product(*[niter, perp, learn]))

    mlist = []
    for par in parcomb:
        mlist.append(TSNE(n_components=2, n_iter=par[0], perplexity=par[1], 
			  learning_rate=par[2], metric="precomputed").fit_transform(m)) 

    gnps = gnps[['cluster index', 'parent mass', 'LibraryID']]
    gnps = pd.merge(gnps, anno, left_on='cluster index', right_on='cluster index', how='left')
    gnps.fillna('', inplace=True) 
    gnps.index = gnps['cluster index'] 
    gnps = gnps.loc[list(ndict.keys())]

    laccuracy = []
    with PdfPages('silhouette_test.pdf') as pdf: 
        for ii in range(len(mlist)):
            X = mlist[ii]
            ncs = get_cluster(X, nclust)
            caccuracy = get_caccuracy(ncs, gnps, metac, method='mean') 
            gnps['clabel_%s' % ii] = ncs['cluster_labels'][caccuracy['maxCaccuracyN']-2] 
            caccuracy['n_iter'] = parcomb[ii][0]
            caccuracy['perplexity'] = parcomb[ii][1]
            caccuracy['learning_rate'] = parcomb[ii][2]
            laccuracy.append(caccuracy)
            fig, ax1, ax2 = plot_silhouette(X, caccuracy['maxCaccuracyN'], caccuracy['maxCaccuracySilhoutte'],
	                                    parcomb[ii], show=False) 
            pdf.savefig()  # saves the current figure into a pdf page 
            plt.close() 
    
    pd.DataFrame(laccuracy).to_csv(staskid+'_clusterng_accuracy.tsv', sep='\t', index=None)
    gnps.to_csv(staskid+'_gnps_cluster_labels.tsv', sep='\t', index=None)

if __name__ == '__main__':
    start_time = time.time()
    main()
    print("--- %s seconds ---" % (time.time() - start_time))

